{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intermediate Slurm on Milton\n",
    "### Delivered by WEHI Research Computing Platform\n",
    "\n",
    "<pre>Edward Yang    Michael Milton    Julie Iskander</pre><br>\n",
    "\n",
    "<img src=\"static/1200px-Slurm_logo.svg.png\" alt=\"Slurm\" width=\"100\"/>\n",
    "<img src=\"static/milton.png\" alt=\"Milton Mascot\" width=\"100\"/>\n",
    "<img src=\"static/WEHI_RGB_logo.png\" alt=\"WEHI logo\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Acknowledgment of Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction and Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Introduction and Housekeeping 1</div>\n",
    "\n",
    "## Self Introductions!\n",
    "\n",
    "* Your Name\n",
    "* What you're using Milton's HPC for\n",
    "* Your coffee/tea order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Introduction and Housekeeping 2</div>\n",
    "\n",
    "## Agenda for Today\n",
    "\n",
    "1. <span style=\"color: blue\">Introduction & Housekeeping</span>\n",
    "\n",
    "2. Laying the groundwork: nodes, tasks and other Slurm terminology\n",
    "\n",
    "3. Understanding your jobs and the cluster\n",
    "\n",
    "4. Lunch (30mins)\n",
    "\n",
    "5. Basic job profiling + 5min break\n",
    "\n",
    "6. Slurm scripting features + 5min break\n",
    "\n",
    "7. Embarrasingly parallel workflows with Slurm job arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Introduction and Housekeeping 3</div>\n",
    "\n",
    "## Background\n",
    "* We already ran an \"intro to Slurm\" workshop (recording on RCP website)\n",
    "* More \"advanced\" features of Slurm were highly requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This workshop tries to explain concepts like \"tasks\", \"nodes\", and \"job steps\"\n",
    "* It also moves beyond basic job submissions and tries to answer questions like:\n",
    "    * How do I get more info on my jobs e.g., resource utilization?\n",
    "    * What other Slurm features can I utulise to improve my workflows?\n",
    "    * How do I handle embarassingly parallel jobs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Target Audience\n",
    "* You've submitted quite a few jobs via `sbatch`\n",
    "* You're familiar with making resource requests. Like:\n",
    "    * using `--ntasks` and `--cpus-per-task`\n",
    "    * using `--mem` and/or `--memory-per-cpu`\n",
    "* You're wondering whether your jobs are utilizing resources efficiently\n",
    "* You're wondering how to make your life easier when using `sbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Introduction and Housekeeping 4</div>\n",
    "\n",
    "### Expected understanding\n",
    "* Awareness of \"resources\": CPUs, RAM/memory, Nodes, gres (GPUs)\n",
    "\n",
    "* Have used job submission commands\n",
    "    * `srun    # executes a command/script/binary across tasks`\n",
    "    * `salloc  # allocates resources to be used (interactively and/or via srun)`\n",
    "    * `sbatch  # submits a script for later execution on requested resources`\n",
    "\n",
    "* awareness of resource request options\n",
    "    * `--ntasks=             # \"tasks\" recognised by srun`\n",
    "    * `--nodes=              # no. of nodes`\n",
    "    * `--ntasks-per-node=    # tasks per node`\n",
    "    * `--cpus-per-task=      # cpus per task`\n",
    "    * `--mem=                # memory required for entire job`\n",
    "    * `--mem-per-cpu=        # memory required for each CPU`\n",
    "    * `--gres=               # \"general resource\" (i.e. GPUs)`\n",
    "    * `--time=               # requested wall time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Introduction and Housekeeping 5</div>\n",
    "\n",
    "### Format\n",
    "Slides + live coding\n",
    "\n",
    "Live coding will be on Milton, so make sure you're connected to WEHI's VPN or staff network, or use RAP: https://rap.wehi.edu.au\n",
    "\n",
    "Please follow along to reinforce learning!\n",
    "\n",
    "Questions:\n",
    "* Put your hand up whenever you have a question or have an issue running things\n",
    "* Questions in the chat are welcome and will be addressed by helpers\n",
    "\n",
    "Material is available here: https://github.com/WEHI-ResearchComputing/intermediate-slurm-workshop\n",
    "* Link to webpage in the README\n",
    "* example scripts are in demo-scripts folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laying the Groundwork: Nodes, Tasks and Other Slurm Stuff\n",
    "\n",
    "Reviewing cluster concepts and explaining **tasks**, and **srun** Slurm concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 1</div>\n",
    "\n",
    "### What are Nodes?\n",
    "Nodes are essentially standalone computers with their own CPU cores, RAM, local storage, and maybe GPUs. \n",
    "\n",
    "**Note**: Slurm calls CPU cores CPUs (e.g. `cpus-per-tasks`).\n",
    "\n",
    "<img src=\"static/node-diagram.png\" alt=\"Node diagram\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 2</div>\n",
    "\n",
    "HPC clusters (or just clusters) will consist of multiple nodes connected together through a (sometimes fast) network. <br>\n",
    "<img src=\"static/cluster-diagram.png\" alt=\"Cluster diagram\" width=\"400\">\n",
    "<img src=\"static/shared-storage-diagram.png\" alt=\"Shared storage diagram\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 3</div>\n",
    "\n",
    "Typically, HPC is organised with login nodes, compute nodes, and some nodes that perform scheduling and storage duties.<br>\n",
    "<img src=\"static/simple-cluster-diagram.png\" alt=\"Example cluster architecture\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 4</div>\n",
    "\n",
    "<img src=\"static/simple-cluster-diagram-sshslurmlogin.png\" alt=\"ssh to login node\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 5</div>\n",
    "\n",
    "### A review of launching jobs (`srun` vs `sbatch` and `salloc`)\n",
    "\n",
    "TL;DR:\n",
    "* `sbatch` requests resources for use with a script\n",
    "* `salloc` requests resources to be used interactively\n",
    "* `srun` runs programs/scripts using resources requested by `sbatch` and `salloc`\n",
    "\n",
    "`srun` will execute `ntasks` instances of the same command/script/program.\n",
    "\n",
    "Example:\n",
    "1. Try requesting an interactive session with `salloc --ntasks=4 --cpus-per-task=2 --nodes=4`\n",
    "    * Requests 4 tasks with 2 CPU cores each. Each task is located on a different node.\n",
    "2. Then, execute the `hostname` command\n",
    "3. Try `hostname`, but with `srun` i.e. `srun hostname`\n",
    "4. Exit your `salloc` session and try run `srun --ntasks=4 --cpus-per-task=2 --nodes=4 hostname`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 6</div>\n",
    "\n",
    "<img src=\"static/simple-cluster-diagram-sbatchscript.png\" alt=\"Example sbatch command\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 7</div>\n",
    "\n",
    "<img src=\"static/simple-cluster-diagram-sbatchnodes2script.png\" alt=\"Example sbatch command\" width=\"800\"><br>\n",
    "\n",
    "The `BatchHost` executes the commands in `script.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 8</div>\n",
    "\n",
    "### How does `sbatch/salloc` and `srun` work together?\n",
    "<img src=\"static/salloc-3node-diagram.png\" alt=\"salloc requesting 3 nodes diagram\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 9</div>\n",
    "\n",
    "A command/script/program will be executed on `BatchHost` **only**\n",
    "\n",
    "<img src=\"static/salloc-3node-diagram-hostname.png\" alt=\"hostname command run on 3 nodes\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 10</div>\n",
    "\n",
    "To use the other nodes, execute `srun hostname`.\n",
    "\n",
    "`BatchHost` will send the `hostname` command to the remaining tasks to be executed as well.\n",
    "\n",
    "<img src=\"static/salloc-3node-diagram-srunhostname.png\" alt=\"hostname command run on 3 nodes\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 11</div>\n",
    "\n",
    "### Requesting multiple nodes doesn't guarantee they will all be used!\n",
    "Without `srun`, only the `BatchHost` executes the commands/script\n",
    "\n",
    "Using `srun` still doesn't guarantee the extra nodes will be used \"properly\"!\n",
    "\n",
    "Nodes cannot collaborate on problems unless they are running a program designed that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's like clicking your mouse on your PC, and expecting the click to register on a colleague's PC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's possible, but needs a special program/protocol to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Biological sciences and statistics tend not to make use of multiple nodes to cooperate on a single problem.\n",
    "\n",
    "Hence, we recommend passing `--nodes=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 12</div>\n",
    "\n",
    "### What are tasks?\n",
    "Tasks are a collection of resources (CPU cores, GPUs) expected to perform the same \"task\", or used by a single program e.g., via threads, Python multiprocessing, or OpenMP.\n",
    "\n",
    "Tasks not equivalent to no. of CPUs!<br>\n",
    "<img src=\"static/tasks-diagram1.png\" alt=\"Tasks diagram\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 13</div>\n",
    "\n",
    "### So why are tasks useful/important?\n",
    "\n",
    "The Slurm task model was created with \"traditional HPC\" in mind\n",
    "* `srun` creates `ntasks` instances of a program which coordinate using MPI\n",
    "* Some applications are designed to use multiple cores per task (hybrid MPI-OpenMP) for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tasks are not as relevant in bioinformatics, but Slurm nevertheless uses tasks for accounting/profiling purposes. \n",
    "\n",
    "Therefore, it's useful to have an understanding of tasks in order to interpret some of Slurm's job accounting/profiling outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Nodes, Tasks and Other Slurm Stuff 14</div>\n",
    "\n",
    "A task can only be given resources co-located on a node. <br>\n",
    "Multiple tasks requested by `sbatch` or `salloc` can be spread across multiple nodes (unless `--nodes=` is specified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, if we have two nodes with 4 CPU cores each:\n",
    "\n",
    "requesting 1 task and 8 cpus-per-task won't work.\n",
    "\n",
    "But requesting 2 tasks and 4 cpus-per-task will!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most data science, statistics, bionformatics, health-science work will use `--ntasks=1`, and using `--cpus-per-task`. \n",
    "\n",
    "If you see/hear anything to do with \"distributed\" or MPI (e.g. distributed ML), you may want to change these options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monitoring Your Jobs and the Cluster\n",
    "\n",
    "Using Slurm and system tools to understand what your jobs are doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 1</div>\n",
    "Slurm has lots of data on your jobs and the cluster!\n",
    "\n",
    "Primary utilities discussed in this section:\n",
    "* `squeue`    _Live_ Job queue data (queries the Slurm controller directly)\n",
    "* `sacct`     _Historical_ job data (queries the Slurm database)\n",
    "* `scontrol`  _Live_ Singular job data (queries the Slurm controller directly)\n",
    "* `sinfo`     _Live_ Cluster data (queries the Slurm controller directly)\n",
    "\n",
    "This section show you how to get more detailed information about:\n",
    "* the queue,\n",
    "* other jobs in the queue, and\n",
    "* the state/business of the cluster.\n",
    "\n",
    "WEHI also offers the HPC dashboard which provide visibility on the status of the cluster.<br>\n",
    "http://dashboards.hpc.wehi.edu.au/\n",
    "\n",
    "**Note**: the dashboards' info is coarser than what the Slurm commands can provide, and is specific to WEHI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 2</div>\n",
    "\n",
    "### Building on the basics: `squeue`\n",
    "\n",
    "`squeue` shows everyone's job in the queue (passing `-u <username>`) shows only `<username>`'s jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           8516030      gpuq interact bollands  R    1:30:11      1 gpu-p100-n01\n",
      "           8515707      gpuq cryospar cryospar  R    3:04:59      1 gpu-p100-n01\n",
      "           8511988 interacti sys/dash    yan.a  R   20:15:53      1 sml-n03\n",
      "           8516092 interacti     work jackson.  R    1:21:42      1 sml-n01\n"
     ]
    }
   ],
   "source": [
    "squeue | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 3</div>\n",
    "\n",
    "But what if we want _even more_ information?\n",
    "\n",
    "We have to make use of the formatting options!\n",
    "\n",
    "```\n",
    "$ squeue --Format field1,field2,...\n",
    "```\n",
    "\n",
    "OR use the environment variable `SQUEUE_FORMAT2`. Useful fields:\n",
    "\n",
    "| Resources related | Time related | Scheduling   |\n",
    "| :---              | :---         | :---         |\n",
    "| `NumCPUs`         | `starttime`  | `JobId`      |\n",
    "| `NumNodes`        | `submittime` | `name`       |\n",
    "| `minmemory`       | `pendingtime`| `partition`  |\n",
    "| `tres-alloc`      | `timelimit`  | `priority`   |\n",
    "| `minmemory`       | `timeleft`   | `reasonlist` |\n",
    "|                   | `timeused`   | `workdir`    |\n",
    "|                   |              | `state`      |\n",
    "\n",
    "You can always use `man squeue` to see the entire list of options.\n",
    "\n",
    "So you don't have to type out the fields, I recommend aliasing the the command with your fields of choice in `~/.bashrc` e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 4</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID   NAME   PARTITION  ST TRES_ALLOC                                                  TIME_LIMIT  TIME_LEFT   \n",
      "8517002 R      bigmem     R  cpu=22,mem=88G,node=1,billing=720984                        1-00:00:00  23:35:18    \n",
      "8516030 intera gpuq       R  cpu=2,mem=20G,node=1,billing=44,gres/gpu=1,gres/gpu:p100=1  8:00:00     4:43:00     \n",
      "8515707 cryosp gpuq       R  cpu=8,mem=17G,node=1,billing=44,gres/gpu=1,gres/gpu:p100=1  2-00:00:00  1-19:08:12  \n",
      "8511988 sys/da interactiv R  cpu=8,mem=16G,node=1,billing=112                            1-00:00:00  1:57:18     \n"
     ]
    }
   ],
   "source": [
    "alias sqv=\"squeue --Format=jobid:8,name:6' ',partition:10' ',statecompact:3,tres-alloc:60,timelimit:12,timeleft:12\"\n",
    "sqv | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID   NAME   PARTITION  ST TRES_ALLOC                                                  TIME_LIMIT  TIME_LEFT   \n",
      "8516851 bionix regular    PD cpu=24,mem=90G,node=1,billing=204                           2-00:00:00  2-00:00:00  \n",
      "8516850 bionix regular    PD cpu=24,mem=90G,node=1,billing=204                           2-00:00:00  2-00:00:00  \n",
      "8516849 bionix regular    PD cpu=24,mem=90G,node=1,billing=204                           2-00:00:00  2-00:00:00  \n",
      "8516848 bionix regular    PD cpu=24,mem=90G,node=1,billing=204                           2-00:00:00  2-00:00:00  \n"
     ]
    }
   ],
   "source": [
    "sqv -u bedo.j | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 5</div>\n",
    "\n",
    "### Getting detailed information of your running/pending job\n",
    "\n",
    "`scontrol show job <jobid>`\n",
    "\n",
    "Useful if you care only about a specific job.\n",
    "\n",
    "It's very useful when debugging jobs.\n",
    "\n",
    "A lot of information without needing lots of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId=8516360 JobName=Extr16S23S\n",
      "   UserId=woodruff.c(2317) GroupId=allstaff(10908) MCS_label=N/A\n",
      "   Priority=324 Nice=0 Account=wehi QOS=normal\n",
      "   JobState=RUNNING Reason=None Dependency=(null)\n",
      "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
      "   RunTime=00:21:53 TimeLimit=2-00:00:00 TimeMin=N/A\n",
      "   SubmitTime=2022-10-20T11:37:49 EligibleTime=2022-10-20T11:37:49\n",
      "   AccrueTime=2022-10-20T11:37:49\n",
      "   StartTime=2022-10-20T14:28:03 EndTime=2022-10-22T14:28:03 Deadline=N/A\n",
      "   PreemptEligibleTime=2022-10-20T14:28:03 PreemptTime=None\n",
      "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-10-20T14:28:03 Scheduler=Main\n",
      "   Partition=regular AllocNode:Sid=vc7-shared:12938\n",
      "   ReqNodeList=(null) ExcNodeList=(null)\n",
      "   NodeList=med-n24\n",
      "   BatchHost=med-n24\n",
      "   NumNodes=1 NumCPUs=32 NumTasks=32 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n",
      "   TRES=cpu=32,mem=48G,node=1,billing=128\n",
      "   Socks/Node=* NtasksPerN:B:S:C=32:0:*:* CoreSpec=*\n",
      "   MinCPUsNode=32 MinMemoryNode=48G MinTmpDiskNode=0\n",
      "   Features=(null) DelayBoot=00:00:00\n",
      "   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n",
      "   Command=/stornext/Bioinf/data/lab_speed/cjw/microbiome/scripts/shell/ribosomal_16S23S_extract_singlespecies.sh Staphylococcus epidermidis 32\n",
      "   WorkDir=/stornext/Bioinf/data/lab_speed/cjw/microbiome/scripts/shell\n",
      "   StdErr=/stornext/Bioinf/data/lab_speed/cjw/microbiome/scripts/shell/slurm-8516360.out\n",
      "   StdIn=/dev/null\n",
      "   StdOut=/stornext/Bioinf/data/lab_speed/cjw/microbiome/scripts/shell/slurm-8516360.out\n",
      "   Power=\n",
      "   MailUser=woodruff.c@wehi.edu.au MailType=END,FAIL\n",
      "   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scontrol show job 8516360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 6</div>\n",
    "\n",
    "### Information on your _past_ jobs with `sacct`\n",
    "`squeue` and `scontrol show job` only show information on jobs that are in the queue i.e. jobs that are pending, running, or finishing up.\n",
    "\n",
    "Once jobs complete, fail, or are cancelled, the job data is put into a Slurm job data base.\n",
    "\n",
    "This database can be queried by `sacct` to get information about your jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "9041674        crest0.3    regular       wehi        224 CANCELLED+      0:0 \n",
      "9041674.bat+      batch                  wehi         56  CANCELLED     0:15 \n",
      "9041674.ext+     extern                  wehi        224  COMPLETED      0:0 \n",
      "9041674.0         orted                  wehi        168     FAILED      1:0 \n",
      "9170758      gatk-4.2.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9170758.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9170758.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221903      impute_1.+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221903.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9221903.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9221905      lambda.r_+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221905.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9221905.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9221907      limma_3.5+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221907.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9221907.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9221909      listenv_0+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221909.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9221909.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9221910      marray_1.+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221910.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9221910.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9221911      matrixSta+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221911.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9221911.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9221912      parallell+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221912.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9221912.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9221913      r-BH-1.78+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221913.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221913.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221930      sys/dashb+ interacti+       wehi          2    RUNNING      0:0 \n",
      "9221930.bat+      batch                  wehi          2    RUNNING      0:0 \n",
      "9221930.ext+     extern                  wehi          2    RUNNING      0:0 \n",
      "9221945      r-BiocGen+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221945.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221945.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221946      r-GenomeI+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221946.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221946.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221947      r-Biobase+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221947.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221947.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221949      r-R.metho+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221949.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221949.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221950      r-S4Vecto+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221950.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221950.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221955      r-R.oo-1.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221955.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221955.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221956      r-BiocIO-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221956.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221956.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221957      r-IRanges+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221957.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221957.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221958      r-R.utils+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221958.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221958.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221964      r-XML-3.9+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221964.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221964.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221970      r-bitops-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221970.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221970.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221972      r-formatR+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221972.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221972.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221973      r-RCurl-1+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221973.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221973.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221977      r-futile.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221977.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221977.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221978      r-GenomeI+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221978.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221978.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221981      r-globals+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221981.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221981.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221983      r-impute-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221983.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221983.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221985      r-lambda.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221985.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221985.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221986      r-limma-3+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221986.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221986.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221989      r-futile.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221989.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221989.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221990      r-listenv+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221990.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221990.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221992      r-marray-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221992.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221992.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9221999      r-matrixS+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221999.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9221999.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222004      r-CGHbase+    regular       wehi         56  COMPLETED      0:0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9222004.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222004.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222005      r-MatrixG+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222005.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222005.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222006      r-paralle+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222006.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222006.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222007      r-Delayed+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222007.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222007.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222009      r-future-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222009.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222009.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222010      restfulr_+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222010.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222010.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222011      r-future.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222011.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222011.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222012      rjson_0.2+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222012.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222012.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222014      rtracklay+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222014.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222014.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222016      r-rjson-0+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222016.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222016.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222019      snow_0.4-+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222019.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222019.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222020      snowfall_+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222020.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222020.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222022      r-snow-0.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222022.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222022.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222024          source    regular       wehi          2  COMPLETED      0:0 \n",
      "9222024.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222024.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222183      r-BiocPar+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222183.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222183.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222214        kent-404    regular       wehi         56  COMPLETED      0:0 \n",
      "9222214.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222214.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222256      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222256.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222256.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222257      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222257.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222257.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222258      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222258.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222258.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222259      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222259.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222259.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222260      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222260.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222260.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222261      libcap-st+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222261.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222261.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222262      nix-2.5pr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222262.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222262.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222264      bubblewra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222264.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222264.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222265       arx-0.3.2    regular       wehi          6  COMPLETED      0:0 \n",
      "9222265.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222265.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222271      r-snowfal+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222271.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222271.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222331         splitFA    regular       wehi         56  COMPLETED      0:0 \n",
      "9222331.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222331.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222334      r-CGHcall+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222334.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222334.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222341        seed.txt    regular       wehi         56  COMPLETED      0:0 \n",
      "9222341.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222341.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222374      strip-sto+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222374.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222374.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222400      forgeBSge+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222400.ext+     extern                  wehi         56  COMPLETED      0:0 \n",
      "9222400.0    nix-user-+                  wehi         56  COMPLETED      0:0 \n",
      "9222431      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222431.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222431.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222486      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222486.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222486.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222654      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222654.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222654.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222700      slurm-nix+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222700.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222700.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222701      build-bun+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222701.ext+     extern                  wehi          6  COMPLETED      0:0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9222701.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222702      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222702.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222702.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222704      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222704.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222704.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222705      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222705.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222705.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222706      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222706.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222706.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222707      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222707.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222707.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222708      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222708.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222708.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222709      slurm-nix+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222709.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222709.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222711      build-bun+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222711.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222711.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222712      dorado-A1+ gpuq_large       wehi          4     FAILED    127:0 \n",
      "9222712.bat+      batch                  wehi          4     FAILED    127:0 \n",
      "9222712.ext+     extern                  wehi          4  COMPLETED      0:0 \n",
      "9222712.0          time                  wehi          4     FAILED    127:0 \n",
      "9222714      dorado-A1+ gpuq_large       wehi          4  COMPLETED      0:0 \n",
      "9222714.bat+      batch                  wehi          4  COMPLETED      0:0 \n",
      "9222714.ext+     extern                  wehi          4  COMPLETED      0:0 \n",
      "9222714.0          time                  wehi          4  COMPLETED      0:0 \n",
      "9222719      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222719.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222719.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222723      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222723.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222723.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222724      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222724.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222724.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222725      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222725.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222725.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222726      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222726.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222726.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222727      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222727.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222727.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222728      libcap-st+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222728.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222728.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222729      nix-2.5pr+    regular       wehi          6     FAILED      2:0 \n",
      "9222729.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222729.0    nix-user-+                  wehi          6     FAILED      2:0 \n",
      "9222730      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222730.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222730.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222733       arx-0.3.2    regular       wehi          6  COMPLETED      0:0 \n",
      "9222733.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222733.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222734      bubblewra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222734.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222734.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222751      nix-2.5pr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222751.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222751.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222763      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222763.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222763.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222764      interacti+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222764.int+ interacti+                  wehi          2  COMPLETED      0:0 \n",
      "9222764.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222764.0          echo                  wehi          2  COMPLETED      0:0 \n",
      "9222764.1          echo                  wehi          2  COMPLETED      0:0 \n",
      "9222764.2          echo                  wehi          2  COMPLETED      0:0 \n",
      "9222764.3          echo                  wehi          2  COMPLETED      0:0 \n",
      "9222787      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222787.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222787.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222790      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222790.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222790.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222796      bionix-ge+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222796.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222796.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222797      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222797.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222797.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222798      bionix-ge+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222798.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222798.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222799      slurm-nix+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222799.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222799.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222800      build-bun+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222800.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222800.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222812      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222812.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222812.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222813      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222813.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222813.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9222816      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222816.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222816.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222817      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222817.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222817.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222820      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222820.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222820.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222822      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222822.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222822.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222826      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222826.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222826.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222843      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222843.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222843.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222845      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222845.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222845.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222856      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222856.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222856.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222860      interacti+    regular       wehi          4  COMPLETED      0:0 \n",
      "9222860.int+ interacti+                  wehi          4  COMPLETED      0:0 \n",
      "9222860.ext+     extern                  wehi          4  COMPLETED      0:0 \n",
      "9222861      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222861.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222861.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222862      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222862.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222862.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222864      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222864.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222864.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9222865      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222865.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222865.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222872      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222872.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9222872.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9222875      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222875.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9222875.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9223289      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9223289.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9223289.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9225143      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9225143.ext+     extern                  wehi          6  COMPLETED      0:0 \n",
      "9225143.0    nix-user-+                  wehi          6  COMPLETED      0:0 \n",
      "9229643      bionix-wi+    regular       wehi          2  COMPLETED      0:0 \n",
      "9229643.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9229643.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9229733      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9229733.ext+     extern                  wehi          2  COMPLETED      0:0 \n",
      "9229733.0    nix-user-+                  wehi          2  COMPLETED      0:0 \n",
      "9229738           bin.R    regular       wehi         56    PENDING      0:0 \n"
     ]
    }
   ],
   "source": [
    "sacct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 7</div>\n",
    "Default behaviour for `sacct` is to return jobs running today.\n",
    "\n",
    "Choose the time-window with `-S <date-time> -E <date-time>`\n",
    "* date-time format: `YYYY-MM-DDhh:mm:ss`\n",
    "    * does not need entire string e.g. `sacct -S 2022-11` is acceptable too.\n",
    "* `-S`: start date-time\n",
    "* `-E`: end date-time\n",
    "\n",
    "**Note**: big/frequent `sacct` queries can occupy and eventually overload the Slurm controller node.\n",
    "\n",
    "`sacct` behaviour can be augmented by `--format`. See `man sacct` for more details.\n",
    "\n",
    "`-X` can be used to group job steps together, but this prevents some statistics like IO and memory from being reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "9041674        crest0.3    regular       wehi        224 CANCELLED+      0:0 \n",
      "9170758      gatk-4.2.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221903      impute_1.+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221905      lambda.r_+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221907      limma_3.5+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221909      listenv_0+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221910      marray_1.+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221911      matrixSta+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221912      parallell+    regular       wehi          2  COMPLETED      0:0 \n",
      "9221913      r-BH-1.78+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221930      sys/dashb+ interacti+       wehi          2    RUNNING      0:0 \n",
      "9221945      r-BiocGen+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221946      r-GenomeI+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221947      r-Biobase+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221949      r-R.metho+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221950      r-S4Vecto+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221955      r-R.oo-1.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221956      r-BiocIO-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221957      r-IRanges+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221958      r-R.utils+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221964      r-XML-3.9+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221970      r-bitops-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221972      r-formatR+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221973      r-RCurl-1+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221977      r-futile.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221978      r-GenomeI+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221981      r-globals+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221983      r-impute-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221985      r-lambda.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221986      r-limma-3+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221989      r-futile.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221990      r-listenv+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221992      r-marray-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9221999      r-matrixS+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222004      r-CGHbase+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222005      r-MatrixG+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222006      r-paralle+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222007      r-Delayed+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222009      r-future-+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222010      restfulr_+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222011      r-future.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222012      rjson_0.2+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222014      rtracklay+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222016      r-rjson-0+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222019      snow_0.4-+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222020      snowfall_+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222022      r-snow-0.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222024          source    regular       wehi          2  COMPLETED      0:0 \n",
      "9222183      r-BiocPar+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222214        kent-404    regular       wehi         56  COMPLETED      0:0 \n",
      "9222256      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222257      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222258      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222259      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222260      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222261      libcap-st+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222262      nix-2.5pr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222264      bubblewra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222265       arx-0.3.2    regular       wehi          6  COMPLETED      0:0 \n",
      "9222271      r-snowfal+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222331         splitFA    regular       wehi         56  COMPLETED      0:0 \n",
      "9222334      r-CGHcall+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222341        seed.txt    regular       wehi         56  COMPLETED      0:0 \n",
      "9222374      strip-sto+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222400      forgeBSge+    regular       wehi         56  COMPLETED      0:0 \n",
      "9222431      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222486      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222654      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222700      slurm-nix+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222701      build-bun+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222702      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222704      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222705      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222706      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222707      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222708      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222709      slurm-nix+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222711      build-bun+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222712      dorado-A1+ gpuq_large       wehi          4     FAILED    127:0 \n",
      "9222714      dorado-A1+ gpuq_large       wehi          4  COMPLETED      0:0 \n",
      "9222719      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222723      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222724      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222725      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222726      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222727      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222728      libcap-st+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222729      nix-2.5pr+    regular       wehi          6     FAILED      2:0 \n",
      "9222730      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222733       arx-0.3.2    regular       wehi          6  COMPLETED      0:0 \n",
      "9222734      bubblewra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222751      nix-2.5pr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222763      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222764      interacti+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222787      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222790      bionix-bw+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222796      bionix-ge+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222797      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222798      bionix-ge+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222799      slurm-nix+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222800      build-bun+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222812      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222813      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222816      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9222817      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222820      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222822      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222826      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222843      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222845      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222856      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222860      interacti+    regular       wehi          4  COMPLETED      0:0 \n",
      "9222861      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222862      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222864      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9222865      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222872      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9222875      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9223289      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9225143      bionix-sa+    regular       wehi          6  COMPLETED      0:0 \n",
      "9229643      bionix-wi+    regular       wehi          2  COMPLETED      0:0 \n",
      "9229733      bionix-sa+    regular       wehi          2  COMPLETED      0:0 \n",
      "9229738           bin.R    regular       wehi         56 CANCELLED+      0:0 \n",
      "9242414      targets.i+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242415           bin.R    regular       wehi         56  COMPLETED      0:0 \n",
      "9242418      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242419      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242420      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242424      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242425      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242426      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242427      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242428      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242429      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242430      slurm-nix+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242431      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242432      build-bun+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242433      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242434      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242435      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242436      bionix-Ha+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242438      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242439      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242440      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242443      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242445      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242449      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242473      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242480      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242482      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242489      yaml_2.2.+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242493      bionix-Co+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242495      r-yaml-2.+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242496      bionix-In+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242497      r-restful+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242498      bionix-Ge+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242499      zlibbioc_+    regular       wehi          2  COMPLETED      0:0 \n",
      "9242505      r-zlibbio+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242506      r-Rhtslib+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242508      bwrap-wra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242509      chroot-wr+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242510      nix-store+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242511      nix-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242512      ssh-wrapp+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242513      libcap-st+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242514      nix-2.5pr+    regular       wehi          6    RUNNING      0:0 \n",
      "9242517       arx-0.3.2    regular       wehi          6  COMPLETED      0:0 \n",
      "9242524      r-XVector+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242525      bubblewra+    regular       wehi          6  COMPLETED      0:0 \n",
      "9242527      r-Biostri+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242535      r-Genomic+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242536      r-Rsamtoo+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242546      r-Summari+    regular       wehi         56  COMPLETED      0:0 \n",
      "9242553      r-QDNAseq+    regular       wehi         56  COMPLETED      0:0 \n",
      "9244975      r-Genomic+    regular       wehi         56  COMPLETED      0:0 \n",
      "9247737      r-rtrackl+    regular       wehi         56  COMPLETED      0:0 \n",
      "9247869      r-BSgenom+    regular       wehi         56  COMPLETED      0:0 \n",
      "9247873      R-4.1.2-w+    regular       wehi         56    PENDING      0:0 \n"
     ]
    }
   ],
   "source": [
    "sacct -X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\"> Monitoring Your jobs and the Cluster 8</div>\n",
    "\n",
    "### Monitoring the cluster\n",
    "\n",
    "Being able to understand the state of the cluster, can help understand why your job might be waiting.\n",
    "\n",
    "Or, you can use the information to your advantage to reduce wait times.\n",
    "\n",
    "To view the state of the cluster, we're going to use the `sinfo` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION        AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
      "interactive         up 1-00:00:00      4    mix med-n03,sml-n[01-03]\n",
      "interactive         up 1-00:00:00      1  alloc med-n02\n",
      "interactive         up 1-00:00:00      1   idle med-n01\n",
      "regular*            up 2-00:00:00     42    mix lrg-n[02-03],med-n[03-05,07-09,12-13,18,20-23,25-27,29-30],sml-n[02-20,22-24]\n",
      "regular*            up 2-00:00:00     13  alloc lrg-n04,med-n[02,06,10-11,14-17,19,24,28],sml-n21\n",
      "long                up 14-00:00:0     40    mix med-n[03-05,07-09,12-13,18,20-23,25-27,29-30],sml-n[02-20,22-24]\n",
      "long                up 14-00:00:0     12  alloc med-n[02,06,10-11,14-17,19,24,28],sml-n21\n",
      "bigmem              up 2-00:00:00      3    mix lrg-n02,med-n[03-04]\n",
      "bigmem              up 2-00:00:00      1  alloc med-n02\n",
      "bigmem              up 2-00:00:00      1   idle lrg-n01\n",
      "gpuq                up 2-00:00:00      1    mix gpu-p100-n01\n",
      "gpuq                up 2-00:00:00     11   idle gpu-a30-n[01-07],gpu-p100-n[02-05]\n",
      "gpuq_interactive    up   12:00:00      1    mix gpu-a10-n01\n",
      "gpuq_large          up 2-00:00:00      3   idle gpu-a100-n[01-03]\n"
     ]
    }
   ],
   "source": [
    "sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 9</div>\n",
    "The `-N` orders information by nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODELIST      NODES        PARTITION STATE \n",
      "gpu-a10-n01       1 gpuq_interactive mix   \n",
      "gpu-a30-n01       1             gpuq idle  \n",
      "gpu-a30-n02       1             gpuq idle  \n",
      "gpu-a30-n03       1             gpuq idle  \n"
     ]
    }
   ],
   "source": [
    "sinfo -N | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Monitoring Your Jobs and the Cluster 10</div>\n",
    "But just knowing whether nodes are \"idle\", \"mixed\", or \"allocated\" is not the _most useful_ information.\n",
    "\n",
    "We can add detail with formatting options as well.\n",
    "\n",
    "| CPU | memory | gres (GPU) | node state | time |\n",
    "| :---| :--- | :--- | :--- | :--- |\n",
    "| `CPUsState` | `FreeMem` | `GresUsed` | `StateCompact` | `Time` |\n",
    "| | `AllocMem` | `Gres` | | |\n",
    "| | `Memory` | | | |\n",
    "\n",
    "* CPUs occupied/available/total: CPUsState\n",
    "* memory occupied/available/total: FreeMem, AllocMem, Memory\n",
    "* gres (GPU) occupied/available: GresUsed, Gres\n",
    "* State of the node (e.g. whether the node is down): StateCompact\n",
    "* Max time: Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODELIST    PARTITION  CPUS(A/I/O/T) FREE_MEM MEMORY   GRES_USED           GRES       STATE   TIMELIMIT           \n",
      "gpu-a10-n01 gpuq_inter 0/48/0/48     163914   257417   gpu:A10:0(IDX:N/A)  gpu:A10:4  idle    12:00:00            \n",
      "gpu-a30-n01 gpuq       0/96/0/96     450325   511362   gpu:A30:0(IDX:N/A)  gpu:A30:4  idle    2-00:00:00          \n",
      "gpu-a30-n02 gpuq       0/96/0/96     436435   511362   gpu:A30:0(IDX:N/A)  gpu:A30:4  idle    2-00:00:00          \n",
      "gpu-a30-n03 gpuq       0/96/0/96     497816   511362   gpu:A30:0(IDX:N/A)  gpu:A30:4  idle    2-00:00:00          \n"
     ]
    }
   ],
   "source": [
    "sinfo -NO nodelist:11' ',partition:10' ',cpusstate:13' ',freemem:8' ',memory:8' ',gresused,gres:11,statecompact:8,time | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Job Profiling\n",
    "\n",
    "Using command-line tools to obtain visibility into how your job is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 1</div>\n",
    "\n",
    "This section will look at using command-line tools to obtain visibility into how your job is performing.\n",
    "\n",
    "| type of data | Live | Historical |\n",
    "| --- | --- | --- |\n",
    "| **good for** | debugging | debugging |\n",
    "| | evaluating utilization | profiling |\n",
    "| **drawbacks** | uses system tools, so requires some system understanding | Only provides data when jobs are completed |\n",
    "\n",
    "We will look at:\n",
    "* `htop` for _Live_ Process activity on nodes\n",
    "* `nvidia-smi` and `nvtop` for _Live_ GPU activity on nodes\n",
    "* `seff` for _Historical_ job CPU and memory usage data\n",
    "* `dcgmstats` for _Historical_ job GPU usage data\n",
    "* `sacct` for _Historical_ job data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 2</div>\n",
    "\n",
    "### Live monitoring of jobs\n",
    "\n",
    "Slurm can't provide accurate \"live\" data about jobs' activities\n",
    "\n",
    "System tools must be used instead.\n",
    "\n",
    "This requires matching jobs to processes on a node with `squeue` and `ssh`.\n",
    "\n",
    "<img src=\"static/simple-cluster-diagram-sshcompute.png\" alt=\"Cluster diagram ssh to node\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 3</div>\n",
    "\n",
    "#### Live monitoring: CPU, memory, IO activity\n",
    "`htop` is a utility often installed on HPC clusters for monitoring processes.\n",
    "\n",
    "It can be used to look at the CPU, memory, and IO utilization of a running process. \n",
    "\n",
    "It's not a Slurm tool, but is nevertheless very useful in monitoring jobs' activity and diagnosing issues.\n",
    "\n",
    "To show only your processes, execute `htop -u $USER`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 4</div>\n",
    "\n",
    "`htop` shows the individual CPU core utilization on the top, followed by memory utilization and some misc. information.\n",
    "\n",
    "The bottom panel shows the process information\n",
    "\n",
    "Relevant Headings:\n",
    "* `USER`: User that owns the process\n",
    "* `PID`: Process ID\n",
    "* `%CPU`: % of a single core that a process is using e.g. 400% means process is using 4 cores\n",
    "* `%MEM`: % of node's total RAM that process is using\n",
    "* `VSZ`: \"Virtual\" memory (bytes) - the memory a process \"thinks\" it's using\n",
    "* `RSS`: \"Resident\" memory (bytes) - the actual physical memory a process is using\n",
    "* `S`: \"State\" of the process\n",
    "    * `D`: \"Uninterruptible\" sleep - waiting for something else, often IO\n",
    "    * `R`: Running\n",
    "    * `S`: Sleeping\n",
    "    * `T`: \"Traced\" or stopped e.g. by a debugger or manually i.e., paused\n",
    "    * `Z`: \"Zombie\" - process has completed and is waiting to clean up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 5</div>\n",
    "You can organize the process information into \"trees\" by pressing `F5`\n",
    "\n",
    "You can add IO information by\n",
    "1. Press `F2` (Setup)\n",
    "2. Press down three times to move the cursor to \"Columns\" and press right twice\n",
    "3. The cursor should now be in \"Available Columns\". Scroll down to `IO_READ_RATE` and press enter\n",
    "4. Scroll down to `IO_WRITE_RATE` and press enter\n",
    "5. Press `F10` to exit. \n",
    "You should now be able to see read/write rates for processes that you have permissions for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<strong>Tips</strong>: \n",
    "* `htop` configurations are saved in `~/.config/htop`. Delete this folder to reset your `htop` conifguration.\n",
    "* `ps` and `pidstat` are useful alternatives which can be incorporated into scripts.\n",
    "* Some systems may not have `htop` installed, in which case `top` can be used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 6</div>\n",
    "\n",
    "#### Live monitoring: GPU activity\n",
    "\n",
    "To monitor activity of Milton's NVIDIA GPUs, we must rely on NVIDIA's `nvidia-smi` tool.\n",
    "\n",
    "`nvidia-smi` shows information about the memory and compute utilization, process allocation and other details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`nvtop` is a command also available on Milton GPU nodes. It works similarly to `htop`.\n",
    "\n",
    "Note that `nvtop` is a third-party tool and is less common, whereas `nvidia-smi` will always be available wherever NVIDIA GPUs are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Like `htop`, `nvidia-smi` and `nvtop` only provides information on processes running on a GPU. If your job is occupying an entire node and all its GPUs, it should be straightforward to determine which GPUs you've been allocated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if your job is sharing a node with other jobs, you might not know straight away which GPU your job has been allocated. You can determine this by\n",
    "* inferring by the command being run on a GPU, or\n",
    "* using `squeue` with extra formatting options as discussed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<strong>Note</strong>:\n",
    "This tool is available only on GPU nodes where the CUDA drivers are installed, so you must `ssh` to a `gpu` node to try it.\n",
    "\n",
    "<strong>Tip</strong>: Combine `nvidia-smi` with `watch` to automatically update the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Monitoring and Profiling 7</div>\n",
    "\n",
    "### Historical monitoring of jobs\n",
    "Slurm tools and plugins are generally easier to use because they provide information on a per-job basis, meaning there's no need to match processes with jobs like previously discussed.\n",
    "\n",
    "<strong>Tips</strong>: _generally_, results are more reliable when executing commands with `srun`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 8</div>\n",
    "\n",
    "#### Historical data: CPU and memory utilization\n",
    "The `seff` command summarizes memory and CPU utilization of a job.\n",
    "\n",
    "It's mainly useful for job steps that have ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 8665813\n",
      "Cluster: milton\n",
      "User/Group: yang.e/allstaff\n",
      "State: COMPLETED (exit code 0)\n",
      "Nodes: 1\n",
      "Cores per node: 4\n",
      "CPU Utilized: 00:09:04\n",
      "CPU Efficiency: 99.27% of 00:09:08 core-walltime\n",
      "Job Wall-clock time: 00:02:17\n",
      "Memory Utilized: 1.95 GB (estimated maximum)\n",
      "Memory Efficiency: 48.83% of 4.00 GB (1.00 GB/core)\n"
     ]
    }
   ],
   "source": [
    "seff 8665813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: `seff` results are not as useful for jobs that have failed or been cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 9</div>\n",
    "\n",
    "In addition to general job information `sacct` can be used to retrieve IO and memory data about _past_jobs\n",
    "\n",
    "Like `squeue`, the default output is a limited, but can be augmented by the `--format` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following `sacct` command shows your job data for jobs since 1st Nov:\n",
    "* Job steps' ID and name\n",
    "* Requested resources\n",
    "* Elapsed time\n",
    "* The quantity of data written and read\n",
    "* The quantity of virtual and resident memory used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the IO and memory values shown will be for the highest use <strong>task</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 10</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         JobID    JobName NCPUS        NodeList    Elapsed      State  MaxDiskRead MaxDiskWrite  MaxVMSize     MaxRSS \n",
      "-------------- ---------- ----- --------------- ---------- ---------- ------------ ------------ ---------- ---------- \n",
      "       8664599 sys/dashb+     2         sml-n01 1-00:00:22    TIMEOUT                                                 \n",
      " 8664599.batch      batch     2         sml-n01 1-00:00:23  CANCELLED      102.64M       15.11M   1760920K     99812K \n",
      "8664599.extern     extern     2         sml-n01 1-00:00:22  COMPLETED        0.00M            0    146612K        68K \n"
     ]
    }
   ],
   "source": [
    "sacct -S 2022-11-01 -o jobid%14' ',jobname,ncpus%5' ',nodelist,elapsed,state,maxdiskread,maxdiskwrite,maxvmsize,maxrss | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 11</div>\n",
    "\n",
    "### A short aside on job _steps_\n",
    "Slurm breaks jobs into steps. Jobs will have steps:\n",
    "* `.extern`: work done not part of the job i.e. overhead\n",
    "* `.<index>`: work done with `srun`\n",
    "* `.batch`: work inside an `sbatch` script, but not executed by `srun`\n",
    "* `.interactive`: work done inside an interactive `salloc` session, but not executed by `srun`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Monitoring and Profiling 12</div>\n",
    "\n",
    "#### Historical data: GPU activity\n",
    "By default, Slurm doesn't have the ability to produce stats on GPU usage.\n",
    "\n",
    "WEHI's ITS have implemented the `dcgmstats` NVIDIA Slurm plugin which can produce these summary stats.\n",
    "\n",
    "To use this plugin, pass the `--comment=dcgmstats` option to `srun`, `salloc`, or `sbatch`.\n",
    "\n",
    "If your job requested at least one GPU, an extra output file will be generated in the working directory called `dcgm-stats-<jobid>.out`. The output file will contain a table for each GPU requested by the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Basic Job Profiling 13</div>\n",
    "\n",
    "### Summary\n",
    "* live monitoring:\n",
    "    * `htop` for CPU, memory, and IO data (requires configuration)\n",
    "    * `nvidia-smi` for GPU activity\n",
    "    * both require matching jobs to hardware and running processes\n",
    "* historical monitoring:\n",
    "    * `seff` command for simple CPU and memory utilization data for one job\n",
    "    * `sacct` command for memory and IO data for multiple past jobs\n",
    "    * `dcgmstats` Slurm plugin for GPU stats for a single Slurm job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sbatch Scripting Features\n",
    "Taking advantage of lesser-known options and environment features to make life easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 1</div>\n",
    "Sbatch scripts have a lot of nice features that extend beyond requesting resources. This section will look at some of these useful features which you can use in your workflows.\n",
    "\n",
    "This section will look at:\n",
    "* getting email notifications\n",
    "* changing `stdout` and `stderr` files\n",
    "* controlling how jobs depend on each other\n",
    "* making use of job environments and interpreters (e.g. python or R)\n",
    "* submitting `sbatch` scripts without a script\n",
    "\n",
    "We're going to start with our simple R script submitted by wrapper sbatch script \n",
    "\n",
    "```\n",
    "demo-scripts/matmul.rscript\n",
    "demo-scripts/submit-matmul.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 2</div>\n",
    "\n",
    "```r\n",
    "## matmul.rscript\n",
    "# multiplies two matrices together and prints how long it takes.\n",
    "\n",
    "print(\"starting the matmul R script!\")\n",
    "nrows = 1e3\n",
    "paste0(\"elem: \", nrows, \"*\", nrows, \" = \", nrows*nrows)\n",
    "\n",
    "# generating matrices\n",
    "M <- matrix(rnorm(nrows*nrows),nrow=nrows)\n",
    "N <- matrix(rnorm(nrows*nrows),nrow=nrows)\n",
    "\n",
    "# start matmul\n",
    "start.time <- Sys.time()\n",
    "invisible(M %*% N)\n",
    "end.time <- Sys.time()\n",
    "\n",
    "# Getting final time and writing to stdout\n",
    "elapsed.time <- difftime(time1=end.time, time2=start.time, units=\"secs\")\n",
    "print(elapsed.time)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "## submit-matmul.sh\n",
    "# Example sbatch script exeucting R script that does a matmul\n",
    "\n",
    "#SBATCH --mem=8G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --time=1-\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "\n",
    "# loading module for R\n",
    "module load R/openBLAS/4.2.1\n",
    "\n",
    "Rscript matmul.rscript\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 4</div>\n",
    "\n",
    "### Email notifications\n",
    "Getting notifications about the status of your Slurm jobs remove the need to `ssh` onto Milton and running `squeue` to get the status of your jobs.\n",
    "\n",
    "Instead, it will notify you when your job state has changed e.g. when it has started or ended.\n",
    "\n",
    "To enable this behaviour, add the following options to your job scripts:\n",
    "```\n",
    "--mail-user=me@gmail.com\n",
    "--mail-type=ALL\n",
    "```\n",
    "This sends emails to `me@gmail.com` when the job state changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you only want to know when your job goes through certain states, e.g. if it fails or is pre-empted but not when it starts or finishes:\n",
    "* BEGIN: job starts\n",
    "* END: job finishes successfully\n",
    "* FAIL: job fails\n",
    "* TIME_LIMIT: job reaches time limit\n",
    "* TIME_LIMIT_50/80/90: job reaches 50%/80%/90% of time limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Excercise: add the `--mail-user` and `--mail-type` options to the `submit-matmul.sh` script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 5</div>\n",
    "\n",
    "```r\n",
    "#!/bin/bash\n",
    "# Example sbatch script running Rscript\n",
    "# Does a matmul\n",
    "# rev1 - email notifications\n",
    "\n",
    "#SBATCH --mem=8G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --time=1-\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mail-user=yang.e@wehi.edu.au\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "# loading module for R\n",
    "module load R/openBLAS/4.2.1\n",
    "\n",
    "Rscript matmul.rscript\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 6</div>\n",
    "\n",
    "### `sbatch` without a script\n",
    "In some cases, one may wish to submit singular commands to the scheduler. `srun` and `salloc` can do this, but they need a terminal attached i.e., if you close your terminal with the `srun` or `salloc` session, then the job fails.\n",
    "\n",
    "the `sbatch --wrap` option allows you to submit a singular command instead of an entire script.\n",
    "\n",
    "This can be useful for testing, or implementing `sbatch` inside a script that manages your workflow.\n",
    "\n",
    "Note that `sbatch --wrap` infers which interpreter to use from your active environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `--wrap` option could replace `submit-matmul.sh` by:\n",
    "```\n",
    "sbatch --ntasks=1 --cpus-per-task=2 --mem=8G --wrap=\"module load R/openBLAS/4.2.1; Rscript matmul.rscript\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 7</div>\n",
    "\n",
    "### a short aside on `stdout` and `stderr`\n",
    "Linux uses has two main \"channels\" to send output messages to. One is \"stdout\" (standard out), and the other is \"stderr\" (standard error).\n",
    "\n",
    "If you have ever used the `|` `>` or `>>` shell scripting features, then you've _redirected_ `stdout` somewhere else e.g., to another command, a file, or the void (`/dev/null`).\n",
    "\n",
    "```bash\n",
    "$ ls dir-that-doesnt-exist\n",
    "ls: cannot access dir-that-doesnt-exist: No such file or directory # this is a stderr output`\n",
    "```\n",
    "\n",
    "```bash\n",
    "$ ls ~\n",
    "bin cache Desktop Downloads ... # this is a stdout output!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 8</div>\n",
    "\n",
    "### Redirecting job's `stderr` and `stdout`\n",
    "By default:\n",
    "* job's working directory is the directory you submitted from\n",
    "* `stdout` is directed to `slurm-<jobid>.out` in the job's working directory\n",
    "* `stderr` is directed to wherever `stdout` is directed to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Redirect `stderr` and `stdout` with `--error` and `--output` options. They work with both relative and absolute paths, e.g.\n",
    "```\n",
    "--error=/dev/null\n",
    "--output=path/to/output.out\n",
    "```\n",
    "where paths are resolved relative to the job's working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Variables can be used, like:\n",
    "* `%j`: job ID\n",
    "* `%x`: job name\n",
    "* `%u`: username\n",
    "* `%t`: task ID i.e., seperate file per task\n",
    "* `%N`: node name i.e., seperate file per nodes in job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch Scripting Features 9</div>\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# Example sbatch script running Rscript\n",
    "# Does a matmul\n",
    "# rev2 - added --output and --error options\n",
    "\n",
    "#SBATCH --mem=8G\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --time=1-\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --mail-user=yang.e@wehi.edu.au\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=logs/matmul-%j.out\n",
    "#SBATCH --error=logs-debug/matmul-%j.err\n",
    "\n",
    "# loading module for R\n",
    "module load R/openBLAS/4.2.1\n",
    "\n",
    "Rscript matmul.rscript\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 10</div>\n",
    "\n",
    "### Using job dependancies\n",
    "Slurm allows for submitted jobs to wait for another job to start or finish before beginning. While probably not as effective as workflow managers like Nextflow, Slurm's job dependencies can still be useful for simple workflows.\n",
    "\n",
    "Make a job dependant on another by passing the `--dependency` option with one of the following values:\n",
    "* `afterok:jobid1:jobid2...` waits for `jobid1`, `jobid2` ... to complete successfully\n",
    "* `afternotok:jobid1:...`                                   \" to fail, timeout, or be cancelled.\n",
    "* `afterany:jobid1:...` \"                                   \" to finish (fail, complete, cancelled).\n",
    "* `after:jobid1:...` \"                                 \" to start or are cancelled.\n",
    "\n",
    "e.g. `--dependency=afterok:12345678` will make the job wait for job `12345678` to complete successfully before starting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 11</div>\n",
    "\n",
    "### Job dependancies example: Breaking up restartable jobs\n",
    "Recursive jobs are _one_ way to work with short QOS time limits.\n",
    "\n",
    "Multiple Slurm jobs are submitted with a sequential dependancy pattern, i.e., the second job depends on the first, the third job depends on the second and so on...\n",
    "\n",
    "Slurm script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --ntasks=1\n",
      "#SBATCH --cpus-per-task=2\n",
      "#SBATCH --mem-per-cpu=2G\n",
      "#SBATCH --time=1\n",
      "sleep 10\n"
     ]
    }
   ],
   "source": [
    "cat demo-scripts/restartable-job.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 12</div>\n",
    "We could submit the Slurm script using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8703619\n",
      "8703620\n",
      "8703621\n",
      "8703622\n",
      "8703623\n",
      "8703624\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           8703594 gpuq_larg interact   yang.e  R      21:18      1 gpu-a100-n01\n",
      "           8701908 interacti sys/dash   yang.e  R    2:07:28      1 sml-n01\n",
      "           8703624   regular test-rec   yang.e PD       0:00      1 (Dependency)\n",
      "           8703623   regular test-rec   yang.e PD       0:00      1 (Dependency)\n",
      "           8703622   regular test-rec   yang.e PD       0:00      1 (Dependency)\n",
      "           8703621   regular test-rec   yang.e PD       0:00      1 (Dependency)\n",
      "           8703620   regular test-rec   yang.e PD       0:00      1 (Dependency)\n",
      "           8703619   regular test-rec   yang.e  R       0:00      1 sml-n05\n",
      "           8703616   regular test-rec   yang.e  R       0:22      1 sml-n02\n"
     ]
    }
   ],
   "source": [
    "# cell to run recursive script\n",
    "SCRIPT=demo-scripts/recursive-job.sh\n",
    "\n",
    "# Initiate the loop\n",
    "prereq_jobid=$(sbatch --parsable $SCRIPT)\n",
    "echo $prereq_jobid\n",
    "\n",
    "# Create 5 more dependant jobs with a loop\n",
    "for i in {1..5}; do\n",
    "    prereq_jobid=$(sbatch --parsable --dependency=afterany:$prereq_jobid $SCRIPT)\n",
    "    echo $prereq_jobid\n",
    "done\n",
    "squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 13</div>\n",
    "Breaking down the loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. `prereq_jobid=$(sbatch --parsable $SCRIPT)`\n",
    "    * Initial submit of recursive-job.sh\n",
    "    * uses the `--parsable` option to get the job id from `sbatch`\n",
    "    * the returned jobid is saved as `prereq_jobid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. `for i in {1..5}; do`\n",
    "    * a bash `for` loop that loops through 1 to 5, where i is the looping variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. `prereq_jobid=$(sbatch --parsable --dependency=afterok:${prereq_jobid} demo-scripts/recursive-job.sh`\n",
    "    * similar to 1.\n",
    "    * adds the `--dependency=afterok:${prereq_jobid}` option to link jobs\n",
    "    * `afterany` may be preferred instead of `afterok`\n",
    "    * the newly submitted jobid overwrites the `prereq_jobid` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 14</div>\n",
    "\n",
    "### Job dependancies example 2: recursive jobs\n",
    "Instead of submitting all the jobs ahead of time, you can have a single Slurm scripts that submits itself until all the work is done (or it fails)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "## recursive-job.sh\n",
    "\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem-per-cpu=4G\n",
    "#SBATCH --time=2-\n",
    "#SBATCH --output=output-%j.log\n",
    "#SBATCH --error=output-%j.log\n",
    "#SBATCH --mail-user=me.m@wehi.edu.au\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "\n",
    "# Submitting a new job that depends on this one\n",
    "sbatch --dependency=afternotok:${SLURM_JOBID} recursive-job.sh\n",
    "\n",
    "# srunning the command\n",
    "srun flye [flags] --resume\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <div style='text-align: right'>Sbatch scripting features 15</div>\n",
    " \n",
    " This job:\n",
    " * submits a dependant/child job at the start\n",
    "     * `afternotok` means the dependant job will only start if the current job **doesn't** complete successfully\n",
    " * The `flye` command is expected to run for as long as it can, up to the 2 day wall time\n",
    " * The dependant job will pick up where the previous one got up to\n",
    " * `mail-type=END,FAIL` sends an email when the job either:\n",
    "     * fails due to error (FAIL)\n",
    "     * completes sucessfully (END)\n",
    "     * but doesn't send an email if the job times out\n",
    " * recursion continues until a job completes successfully\n",
    " * may need intervention if the job repeatedly fails due to error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 16</div>\n",
    "\n",
    "### Making use of job environments and interpreters\n",
    "By default, when you submit a Slurm job, Slurm copies all the environment variables in your environment and adds some extra for the job to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "export VAR1=\"here is some text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "echo $VAR1\n"
     ]
    }
   ],
   "source": [
    "cat demo-scripts/env-vars1.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 8681656\n"
     ]
    }
   ],
   "source": [
    "sbatch demo-scripts/env-vars1.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is some text\n"
     ]
    }
   ],
   "source": [
    "cat slurm-8681656.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<strong>Note</strong>: For reproducibility reasons, a Slurm script that relies on environment variables can be submitted inside a wrapper script which first exports the relevant variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 17</div>\n",
    "Alternatively, you can use the `--export` option which allows you to set specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is some text\n"
     ]
    }
   ],
   "source": [
    "echo $VAR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 8681761\n"
     ]
    }
   ],
   "source": [
    "sbatch --export=VAR1=\"this is some different text\" demo-scripts/env-vars1.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is some different text\n"
     ]
    }
   ],
   "source": [
    "cat slurm-8681761.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This feature is especially useful when submitting jobs inside wrapper scripts.\n",
    "\n",
    "You can also use the `--export-file` option to specify a file with a list of `VAR=value` pairs that you wish the script to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 18</div>\n",
    "Slurm also adds environment variables that enable job parameters use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --ntasks=1\n",
      "#SBATCH --cpus-per-task=2\n",
      "\n",
      "echo I am running on ${SLURM_NODELIST}\n",
      "echo with ${SLURM_NTASKS} tasks\n",
      "echo and ${SLURM_CPUS_PER_TASK} CPUs per task\n"
     ]
    }
   ],
   "source": [
    "cat demo-scripts/env-vars2.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 8681710\n"
     ]
    }
   ],
   "source": [
    "sbatch demo-scripts/env-vars2.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am running on sml-n03\n",
      "with 1 tasks\n",
      "and 2 CPUs per task\n"
     ]
    }
   ],
   "source": [
    "cat slurm-8681710.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These Slurm environment variables make it easy to supply parallelisation parameters to a program e.g. specifying number of threads.\n",
    "\n",
    "**Tip**: scripts/programs executed by `srun` will have a `SLURM_PROCID` environment variable seperating slurm tasks (MPI-like programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 19</div>\n",
    "\n",
    "### Submitting scripts with different interpreters\n",
    "Typically scripts submitted by `sbatch` use the `bash` or `sh` interpreter (e.g. `#!/bin/bash`), but it may be more convenient to use a different interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can do this by changing the \"hash bang\" statement at the top of the script. To demonstrate this, we can take our original R matmul script, and add a \"hash bang\" statement to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```R\n",
    "#!/usr/bin/env Rscript\n",
    "## matmul.rscript\n",
    "\n",
    "print(\"starting the matmul R script!\")\n",
    "nrows = 1e3\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The statement in the above looks for the Rscript in your current environment. This statement only works because Slurm will copy your environment when a Slurm script is submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`python` works similarly. Replace `Rscript` in the hash bang statement to `python`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively, you can specify the absolute path to the interpreter.\n",
    "\n",
    "e.g. `#!/stornext/System/data/apps/R/openBLAS/R-4.2.1/lib64/R/bin/Rscript`\n",
    "\n",
    "<strong>Tip</strong>: you can use `--export=R_LIBS_USER=...` to point Rscript to your libraries (or `PYTHONPATH` for python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='text-align: right'>Sbatch scripting features 20</div>\n",
    "Changing the interpreter still allows you to access the extra Slurm environment variables, but in a way appropriate to the interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "R example:<br>\n",
    "```r\n",
    "slurmtasks <- Sys.getenv(\"SLURM_NTASKS\")\n",
    "```\n",
    "\n",
    "Python example:\n",
    "```python\n",
    "import os\n",
    "slurmtasks = os.getenv('SLURM_NTASKS')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Excercise:\n",
    "Add a \"using \\<ntasks> tasks and \\<cpus-per-task> CPUs per task\" statement to the matmul R script. Submit the script to confirm it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The output should look something like:\n",
    "```\n",
    "[1] \"starting the matmul R script!\"\n",
    "[1] \"using 1 tasks and 2 CPUs per task\"\n",
    "[1] \"elem: 1000*1000 = 1e+06\"\n",
    "Time difference of 0.06340098 secs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Embarrasingly Parallel Workflows\n",
    "Making life easier with job arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Handling Embarrasingly Parallel Workflows 1</div>\n",
    "\n",
    "### Embarrasingly Parallel jobs\n",
    "Embarrasingly parallel computation is computation that can occur in parallel with minimal coordination. This type of parallel computation is _very_ common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples are parameter scans, genomic sequencing, basecalling, folding@home ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Embarassingly parallel problems are facilitated in Slurm by \"array jobs\". Array jobs allows you to use a single script to submit multiple jobs with similar functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The main benefits to using an array job are:\n",
    "* they are easier to program\n",
    "    * each array job is grouped under a single job ID and distinguished by array indices e.g. 1234567_1, 1234567_2, ...\n",
    "    * no need for wrapper scripts/loops\n",
    "    * can make use of Slurm environment variables to coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Slurm handles large array jobs better than many individual jobs\n",
    "    * helps the Slurm process the queue faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Handling Embarrasingly Parallel Workflows 2</div>\n",
    "\n",
    "### Setting up an array job\n",
    "Array jobs are created by adding the `--array=start-end` option. Slurm jobs, AKA \"tasks\", will be created with indices between `start` and `end`. e.g. `--array=1-10` will create tasks with indices 1, 2, ..., 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`start` and `end` values can be within 0 and 1000 (inclusive). Note this is site specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Singular values or discrete lists can also be specific e.g. `--array=1` or `--array=1,3,5,7-10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```R\n",
    "#!/usr/bin/env Rscript\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=4G\n",
    "#SBATCH --array=1-10\n",
    "\n",
    "## matmul.rscript\n",
    "\n",
    "print(\"starting the matmul R script!\")\n",
    "paste(\"using\", Sys.getenv(\"SLURM_NTASKS\"), \"tasks\")\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Handling Embarrasingly Parallel Workflows 3</div>\n",
    "\n",
    "### Using array indices in other SBATCH output and error\n",
    "Slurm augments the default output behaviour of array jobs automatically.\n",
    "\n",
    "If no `--output` option is provided, an array job will produce a an output file `slurm-<jobid>-<arrayindex>.out` for each index in the array.\n",
    "\n",
    "If you specify `--output` and `--error`, then you can use `%A` and `%a` variables, which represent the job index and the array index, respectively.\n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```R\n",
    "#!/usr/bin/env Rscript\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=4G\n",
    "#SBATCH --array=1-10\n",
    "#SBATCH --output=Rmatmul-times-%A-%a.out\n",
    "#SBATCH --error=Rmatmul-times-%A-%a.err\n",
    "\n",
    "## matmul.rscript\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Handling Embarrasingly Parallel Workflows 4</div>\n",
    "\n",
    "### Using array indices in script body\n",
    "Each task in the array can make use of its index to enable parallelism. This is by making use of the `SLURM_ARRAY_TASK_ID` environment variable.\n",
    "\n",
    "Other environment variables are accessible:\n",
    "* `SLURM_ARRAY_JOB_ID` the job Id of the entire job array\n",
    "* `SLURM_ARRAY_TASK_COUNT` the _number_ of tasks in the array\n",
    "* `SLURM_ARRAY_TASK_MAX` the largest ID of tasks in the array\n",
    "* `SLURM_ARRAY_TASK_MIN` the smallest ID of tasks in the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Add a paste statement to the matmul R script that prints the task ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The output of each job task should look something like:\n",
    "```\n",
    "[1] \"starting the matmul R script!\"\n",
    "[1] \"using 1 tasks and 2 CPUs per task\"\n",
    "[1] \"I am job task 1 in an array of 10!\"\n",
    "[1] \"elem: 1000*1000 = 1e+06\"\n",
    "Time difference of 0.06340098 secs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```r\n",
    "#!/usr/bin/env Rscript\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=4G\n",
    "#SBATCH --array=1-10\n",
    "#SBATCH --output=Rmatmul-times-%A-%a.out\n",
    "#SBATCH --error=Rmatmul-times-%A-%a.err\n",
    "\n",
    "## matmul.rscript\n",
    "\n",
    "print(\"starting the matmul R script!\")\n",
    "\n",
    "paste(\"using\", Sys.getenv(\"SLURM_NTASKS\"), \n",
    "      \"tasks and\", \n",
    "      Sys.getenv(\"SLURM_CPUS_PER_TASK\"), \n",
    "      \"CPUs per task\")\n",
    "\n",
    "paste(\"I am job task\", \n",
    "      Sys.getenv(\"SLURM_ARRAY_TASK_ID\"), \n",
    "      \"in an array of\", \n",
    "      Sys.getenv(\"SLURM_ARRAY_TASK_COUNT\"))\n",
    "\n",
    "nrows = 1e3\n",
    "print(paste0(\"elem: \", nrows, \"*\", nrows, \" = \", nrows*nrows))\n",
    "\n",
    "# generating matrices\n",
    "M <- matrix(rnorm(nrows*nrows),nrow=nrows)\n",
    "N <- matrix(rnorm(nrows*nrows),nrow=nrows)\n",
    "\n",
    "# start matmul\n",
    "start.time <- Sys.time()\n",
    "invisible(M %*% N)\n",
    "end.time <- Sys.time()\n",
    "\n",
    "# Getting final time and writing to stdout\n",
    "elapsed.time <- difftime(time1=end.time, time2=start.time, units=\"secs\")\n",
    "print(elapsed.time)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Handling Embarrasingly Parallel Workflows 5</div>\n",
    "Excercise: modify the `nrows` variable to equal 10*taskID \n",
    "\n",
    "hint: you will need the `strtoi` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Your output from job task 1 should look like:\n",
    "```\n",
    "[1] \"starting the matmul R script!\"\n",
    "[1] \"using 1 tasks and 2 CPUs per task\"\n",
    "[1] \"I am job task 1 in an array of 10!\"\n",
    "[1] \"elem: 10*10 = 100\"\n",
    "Time difference of 0.06340098 secs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "```r\n",
    "#!/usr/bin/env Rscript\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --mem=4G\n",
    "#SBATCH --array=1-10\n",
    "#SBATCH --output=Rmatmul-times-%A-%a.out\n",
    "#SBATCH --error=Rmatmul-times-%A-%a.err\n",
    "\n",
    "## matmul.rscript\n",
    "\n",
    "print(\"starting the matmul R script!\")\n",
    "\n",
    "paste(\"using\", Sys.getenv(\"SLURM_NTASKS\"), \n",
    "      \"tasks and\", \n",
    "      Sys.getenv(\"SLURM_CPUS_PER_TASK\"), \n",
    "      \"CPUs per task\")\n",
    "\n",
    "paste(\"I am job task\", \n",
    "      Sys.getenv(\"SLURM_ARRAY_TASK_ID\"), \n",
    "      \"in an array of\", \n",
    "      Sys.getenv(\"SLURM_ARRAY_TASK_COUNT\"))\n",
    "\n",
    "#nrows = 1e3\n",
    "nrows <- 10 * strtoi(Sys.getenv(\"SLURM_ARRAY_TASK_ID\"))\n",
    "\n",
    "print(paste0(\"elem: \", nrows, \"*\", nrows, \" = \", nrows*nrows))\n",
    "\n",
    "# generating matrices\n",
    "M <- matrix(rnorm(nrows*nrows),nrow=nrows)\n",
    "N <- matrix(rnorm(nrows*nrows),nrow=nrows)\n",
    "\n",
    "# start matmul\n",
    "start.time <- Sys.time()\n",
    "invisible(M %*% N)\n",
    "end.time <- Sys.time()\n",
    "\n",
    "# Getting final time and writing to stdout\n",
    "elapsed.time <- difftime(time1=end.time, time2=start.time, units=\"secs\")\n",
    "print(elapsed.time)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Handling Embarrasingly Parallel Workflows 6</div>\n",
    "\n",
    "For workflows requiring input files or parameters, there are multiple ways you can use job arrays:\n",
    "* folders with job array indices with the necessary input files\n",
    "* programmatically generate CSV file with parameters or paths to files where row/column no. correspond to task IDs\n",
    "* if/else or select case statements in the script\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Handling Embarrasingly Parallel Workflows 7</div>\n",
    "\n",
    "### Job arrays and job dependencies\n",
    "What you can't do:\n",
    "* make job array tasks depend on one another\n",
    "\n",
    "What you can do:\n",
    "* make jobs dependent on entire job arrays\n",
    "    * `--dependency=afterok:<jobid>`\n",
    "* make job arrays dependent on other job/job arrays\n",
    "* make jobs dependent on job array tasks\n",
    "    * `--dependency=afterok:<jobid>_<taskid>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Job arrays and email notifications\n",
    "`--mail-type=ALL` will send notifications only for the entire job (not for each job task)\n",
    "\n",
    "passing `ARRAY_TASKS` will send emails for each array task. e.g. `--mail-type=BEGIN,ARRAY_TASKS` will send an email every time a job array task starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Thanks for attending WEHI's first intermediate Slurm workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align: right\">Conclusion 1</div>\n",
    "\n",
    "Please fill out our feedback form:\n",
    "\n",
    "https://forms.office.com/r/rKku8yqR57\n",
    "\n",
    "We use these forms to help decide which workshops to run in the future and improve our current workshops!\n",
    "\n",
    "\n",
    "Contact us at research.computing@wehi.edu.au for any kind of help related to computing and research!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
